
# -*- coding: utf-8 -*-
"""chatbot_with_memory.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uq5d1vBc4CoXqPKzAWazGXU_SphnnQW0


!pip install chromadb==0.4.6
!pip install pydantic==1.10
!pip install sentence-transformers

!pip install langchain
!pip install huggingface_hub

!pip install transformers

!pip install pypdf

"""
from dotenv import load_dotenv 
from langchain import HuggingFaceHub
from langchain.chains import ConversationChain

#loading the API key
import os


# Load environment variables
load_dotenv()


repo_id = 'lmsys/fastchat-t5-3b-v1.0'  # has 3B parameters: https://huggingface.co/lmsys/fastchat-t5-3b-v1.0
llm = HuggingFaceHub(huggingfacehub_api_token=os.environ['HUGGING_FACE_HUB_API_KEY'],
                     repo_id=repo_id,
                     model_kwargs={'temperature':1e-10, 'max_length':32})

query1 = "Hi! My name is Mani and I love cars. I do have some questions for you"
query2 = "I live in US in a city called TORD. Who is the first president?"
query3 = "Who am I?"
query4 = "What is TORD?"

"""### Conversation Buffer memory"""

from langchain.chains.conversation.memory import ConversationBufferMemory

memory = ConversationBufferMemory()
conversation_buf = ConversationChain(
    llm=llm,
    memory=memory)

print("input: ",query1)
conversation_buf.predict(input=query1)

print("input: ",query2)
conversation_buf.predict(input=query2)

memory.load_memory_variables({})

print("input: ",query3)
conversation_buf.predict(input=query3)

print("input: ",query4)
conversation_buf.predict(input=query4)

print(memory.buffer)

"""### Conversation Buffer Window Memory"""

from langchain.memory import ConversationBufferWindowMemory

memory2 = ConversationBufferWindowMemory(k=2)
conversation_buf2 = ConversationChain(
    llm=llm,
    memory=memory2
)

print("input: ",query1)
conversation_buf2.predict(input=query1)

print("input: ",query2)
conversation_buf2.predict(input=query2)

print("input: ",query3)
conversation_buf2.predict(input=query3)

print(memory2.buffer)

"""### Conversation Summary Memory"""

from langchain.memory import ConversationSummaryBufferMemory

memory3 = ConversationSummaryBufferMemory(llm=llm, max_token_limit=80)
conversation_buf3 = ConversationChain(
    llm=llm,
    memory=memory3
)

print("input: ",query1)
conversation_buf3.predict(input=query1)

print("input: ",query2)
conversation_buf3.predict(input=query2)

print("input: ",query3)
conversation_buf3.predict(input=query3)

memory3.load_memory_variables({})




import langchain
import chromadb

import os
import getpass

from langchain.document_loaders import PyPDFLoader  #document loader: https://python.langchain.com/docs/modules/data_connection/document_loaders
from langchain.text_splitter import RecursiveCharacterTextSplitter  #document transformer: text splitter for chunking
from langchain.embeddings import HuggingFaceEmbeddings
from langchain import PromptTemplate
from langchain.vectorstores import Chroma #vector store
from langchain import HuggingFaceHub  #model hub
from langchain.chains import RetrievalQA

from langchain.memory import ConversationBufferMemory

#loading the API key
import getpass
import os

path = "https://arxiv.org/pdf/2309.12756.pdf"
loader = PyPDFLoader(path)
pages = loader.load()

#number of pages
len(pages)

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)
docs = splitter.split_documents(pages)

embeddings = HuggingFaceEmbeddings()
doc_search = Chroma.from_documents(docs, embeddings)

query = "What is McKinsey?"
similar_docs = doc_search.similarity_search(query, k=3)

repo_id = 'lmsys/fastchat-t5-3b-v1.0'  # has 3B parameters: https://huggingface.co/lmsys/fastchat-t5-3b-v1.0
llm = HuggingFaceHub(huggingfacehub_api_token=os.environ['HUGGING_FACE_HUB_API_KEY'],
                     repo_id=repo_id,
                     model_kwargs={'temperature':1e-10, 'max_length':32})

template = """
Use the following context (delimited by <ctx></ctx>) and the chat history (delimited by <hs></hs>) to answer the question:
------
<ctx>
{context}
</ctx>
------
<hs>
{history}
</hs>
------
{question}
Answer:
"""
prompt = PromptTemplate(
    input_variables=["history", "context", "question"],
    template=template,
)

memory = ConversationBufferMemory(
    memory_key="history",
    input_key="question"
)

retrieval_chain = RetrievalQA.from_chain_type(llm,
                                              chain_type='stuff',
                                              retriever=doc_search.as_retriever(),
                                              chain_type_kwargs={
                                                  "prompt": prompt,
                                                  "memory": memory
                                              })

query = "Who is Thomas Woudsma?"
retrieval_chain.run(query)

query = "What is MlOps?"
retrieval_chain.run(query)

memory.load_memory_variables({})

